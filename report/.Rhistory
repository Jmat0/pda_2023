confusionMatrix(predictions, testData$RESPONSE) # 47%
confusionMatrix(predictions2, testData2$RESPONSE) # 50%
# NN Unbalanced, bank
library(nnet)
library(caret)
# Set up cross-validation
set.seed(123)
# Split the data into training and testing sets
train_idx <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
train_data <- bank[train_idx, ]
test_data <- bank[-train_idx, ]
num_folds <- 5  # Number of cross-validation folds
control <- trainControl(method = "cv", number = num_folds)  # Cross-validation control with balanced training
# Define grid of hyperparameter values
node_grid <- c(2, 3, 4, 5)  # Set of number of nodes
decay_grid <- c(0.1, 0.01, 0.001)  # Set of decay values
# Create the tuning grid
tuning_grid <- expand.grid(size = node_grid, decay = decay_grid)
# Perform cross-validation and tune hyperparameters
tuned_model <- train(RESPONSE ~ ., data = train_data,
method = "nnet",
trControl = control,
tuneGrid = tuning_grid,
maxit = 200,
verboseIter = FALSE)
# Find the best hyperparameter values
best_nodes <- tuned_model$bestTune$size
best_decay <- tuned_model$bestTune$decay
# Train the final model using the best hyperparameter values on the entire training set
final_model <- nnet(RESPONSE ~ ., data = train_data, size = best_nodes, decay = best_decay, maxit = 200, trace = FALSE)
# Evaluate the final model on the test set
predicted_probs <- predict(final_model, newdata = test_data)
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
actual_classes <- test_data$RESPONSE
confusion_matrix <- table(predicted_classes, actual_classes)
# Print confusion matrix
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Calculate sensitivity (true positive rate or recall)
sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
# Calculate specificity (true negative rate)
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
# Print the model parameters
cat("Number of nodes:", best_nodes)
cat("Decay value:", best_decay)
# Print the performance metrics
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
# NN Unbalanced, bank_grouped
library(nnet)
library(caret)
# Set up cross-validation
set.seed(123)
# Split the data into training and testing sets
train_idx <- createDataPartition(bank_grouped$RESPONSE, p = 0.7, list = FALSE)
train_data <- bank[train_idx, ]
test_data <- bank[-train_idx, ]
num_folds <- 5  # Number of cross-validation folds
control <- trainControl(method = "cv", number = num_folds)  # Cross-validation control with balanced training
# Define grid of hyperparameter values
node_grid <- c(2, 3, 4, 5)  # Set of number of nodes
decay_grid <- c(0.1, 0.01, 0.001)  # Set of decay values
# Create the tuning grid
tuning_grid <- expand.grid(size = node_grid, decay = decay_grid)
# Perform cross-validation and tune hyperparameters
tuned_model <- train(RESPONSE ~ ., data = train_data,
method = "nnet",
trControl = control,
tuneGrid = tuning_grid,
maxit = 200,
verboseIter = FALSE)
# Find the best hyperparameter values
best_nodes <- tuned_model$bestTune$size
best_decay <- tuned_model$bestTune$decay
# Train the final model using the best hyperparameter values on the entire training set
final_model <- nnet(RESPONSE ~ ., data = train_data, size = best_nodes, decay = best_decay, maxit = 200, trace = FALSE)
# Evaluate the final model on the test set
predicted_probs <- predict(final_model, newdata = test_data)
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
actual_classes <- test_data$RESPONSE
confusion_matrix <- table(predicted_classes, actual_classes)
# Print confusion matrix
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Calculate sensitivity (true positive rate or recall)
sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
# Calculate specificity (true negative rate)
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
# Print the model parameters
cat("Number of nodes:", best_nodes)
cat("Decay value:", best_decay)
# Print the performance metrics
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
# NN Balanced, bank
library(nnet)
library(caret)
# Set up cross-validation
set.seed(123)
# Split the data into training and testing sets
train_idx <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
train_data <- bank[train_idx, ]
test_data <- bank[-train_idx, ]
num_folds <- 5  # Number of cross-validation folds
control <- trainControl(method = "cv", number = num_folds, sampling = "down")  # Cross-validation control with balanced training
# Define grid of hyperparameter values
node_grid <- c(2, 3, 4, 5)  # Set of number of nodes
decay_grid <- c(0.1, 0.01, 0.001)  # Set of decay values
# Create the tuning grid
tuning_grid <- expand.grid(size = node_grid, decay = decay_grid)
# Perform cross-validation and tune hyperparameters
tuned_model <- train(RESPONSE ~ ., data = train_data,
method = "nnet",
trControl = control,
tuneGrid = tuning_grid,
maxit = 200,
verboseIter = FALSE)
# Find the best hyperparameter values
best_nodes <- tuned_model$bestTune$size
best_decay <- tuned_model$bestTune$decay
# Train the final model using the best hyperparameter values on the entire training set
final_model <- nnet(RESPONSE ~ ., data = train_data, size = best_nodes, decay = best_decay, maxit = 200, trace = FALSE)
# Evaluate the final model on the test set
predicted_probs <- predict(final_model, newdata = test_data)
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
actual_classes <- test_data$RESPONSE
confusion_matrix <- table(predicted_classes, actual_classes)
# Print confusion matrix
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Calculate sensitivity (true positive rate or recall)
sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
# Calculate specificity (true negative rate)
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
# Print the model parameters
cat("Number of nodes:", best_nodes)
cat("Decay value:", best_decay)
# Print the performance metrics
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
# NN balanced, bank_grouped
library(nnet)
library(caret)
# Set up cross-validation
set.seed(123)
# Split the data into training and testing sets
train_idx <- createDataPartition(bank_grouped$RESPONSE, p = 0.7, list = FALSE)
train_data <- bank[train_idx, ]
test_data <- bank[-train_idx, ]
num_folds <- 5  # Number of cross-validation folds
control <- trainControl(method = "cv", number = num_folds, sampling = "down")  # Cross-validation control with balanced training
# Define grid of hyperparameter values
node_grid <- c(2, 3, 4, 5)  # Set of number of nodes
decay_grid <- c(0.1, 0.01, 0.001)  # Set of decay values
# Create the tuning grid
tuning_grid <- expand.grid(size = node_grid, decay = decay_grid)
# Perform cross-validation and tune hyperparameters
tuned_model <- train(RESPONSE ~ ., data = train_data,
method = "nnet",
trControl = control,
tuneGrid = tuning_grid,
maxit = 200,
verboseIter = FALSE)
# Find the best hyperparameter values
best_nodes <- tuned_model$bestTune$size
best_decay <- tuned_model$bestTune$decay
# Train the final model using the best hyperparameter values on the entire training set
final_model <- nnet(RESPONSE ~ ., data = train_data, size = best_nodes, decay = best_decay, maxit = 200, trace = FALSE)
# Evaluate the final model on the test set
predicted_probs <- predict(final_model, newdata = test_data)
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
actual_classes <- test_data$RESPONSE
confusion_matrix <- table(predicted_classes, actual_classes)
# Print confusion matrix
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Calculate sensitivity (true positive rate or recall)
sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
# Calculate specificity (true negative rate)
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
# Print the model parameters
cat("Number of nodes:", best_nodes)
cat("Decay value:", best_decay)
# Print the performance metrics
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
# GLMNET Unbalanced
library(caret)
library(glmnet)
# Split the dataset into training and testing sets
set.seed(123)
trainIndex1 <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
trainData1 <- bank[trainIndex1, ]
testData1 <- bank[-trainIndex1, ]
trainIndex2 <- createDataPartition(bank_grouped$RESPONSE, p = 0.7, list = FALSE)
trainData2 <- bank_grouped[trainIndex2, ]
testData2 <- bank_grouped[-trainIndex2, ]
# Specify the training control settings
ctrlG1 <- trainControl(method = "cv",   # Cross-validation
number = 10,     # Number of folds
verboseIter = FALSE)  # Don't print results
ctrlG2 <- trainControl(method = "cv",
number = 10,
verboseIter = FALSE)
modelG1 <- train(RESPONSE ~ .,       # Formula for the model
data = trainData1,  # Training data
method = "glmnet",  # Model to train
trControl = ctrlG1)  # Training control
modelG2 <- train(RESPONSE ~ .,
data = trainData2,
method = "glmnet",
trControl = ctrlG2)
# Make predictions on new data
predictionsG1 <- predict(modelG1, newdata = testData1)
predictionsG2 <- predict(modelG2, newdata = testData2)
# Create confusion matrices
confusion_matrix_G1 <- confusionMatrix(predictionsG1, testData1$RESPONSE)
confusion_matrix_G2 <- confusionMatrix(predictionsG2, testData2$RESPONSE)
# Print confusion matrices
print(confusion_matrix_G1)
print(confusion_matrix_G2)
# Access lambda values
lambda_valuesG1 <- modelG1$finalModel$lambda
lambda_valuesG2 <- modelG2$finalModel$lambda
# Print best lambda values
best_lambda_G1 <- modelG1$bestTune$lambda
best_lambda_G2 <- modelG2$bestTune$lambda
cat("Best Lambda (Model G1):", best_lambda_G1, "\n")
cat("Best Lambda (Model G2):", best_lambda_G2, "\n")
# Perform permutation-based variable importance
var_impG1 <- varImp(modelG1, scale = FALSE, type = 1)
var_impG2 <- varImp(modelG2, scale = FALSE, type = 1)
# Output lambda values and variable importance
for (i in 1:ncol(bank) - 1) {   # Exclude the response variable from iteration
var_nameG1 <- colnames(bank)[i]
lambda_valG1 <- lambda_valuesG1[i]
importanceG1 <- var_impG1$importance[var_nameG1,]
cat("Variable:", var_nameG1, "\n")
cat("Lambda value:", lambda_valG1, "\n")
cat("Variable Importance:", importanceG1, "\n")
cat("\n")
}
for (i in 1:ncol(bank_grouped) -1) {
var_nameG2 <- colnames(bank_grouped)[i]
lambda_valG2 <- lambda_valuesG2[i]
importanceG2 <- var_impG2$importance[var_nameG2,]
cat("-------------------------------")
cat("Variable:", var_nameG2, "\n")
cat("Lambda value:", lambda_valG2, "\n")
cat("Variable Importance:", importanceG2, "\n")
}
# Print the best model's tuning parameters
print(modelG1$bestTune)
print(modelG2$bestTune)
# Print the best model's performance
print(modelG1$results)
print(modelG2$results)
# Print the predictions
print(predictionsG1)
print(predictionsG2)
# Plot the models
plot(modelG1)
plot(modelG2)
data(bank)
# Split the dataset into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
trainDataB <- bank[trainIndex, ]
testDataB <- bank[-trainIndex, ]
# Split the dataset into training and testing sets
set.seed(123)
trainIndex1 <- createDataPartition(bank_grouped$RESPONSE, p = 0.7, list = FALSE)
trainDataBG <- bank_grouped[trainIndex1, ]
testDataBG <- bank_grouped[-trainIndex1, ]
# Plot the randomForest
plot(rf_model_1)
plot(rf_model_2)
# Plot the confusion matrix on the training
rf_model_1$confusion
rf_model_2$confusion
# Plot the confusion matrix on the testing to check how the model works for unseen data for both models and compare sensitivity
predictions_1 <- predict(rf_model_1, newdata = testDataB)
confusionMatrix(predictions_1, testDataB$RESPONSE) # 64.44%
predictions_2 <- predict(rf_model_2, newdata = testDataBG)
confusionMatrix(predictions_2, testDataBG$RESPONSE) # 95.31%
# Compute the F1 on the testing and training confusion matrices
confusion_1 <- rf_model_1$confusion
confusion_2 <- rf_model_2$confusion
precision_1 <- confusion_1[2, 2] / sum(confusion_1[, 2])
recall_1 <- confusion_1[2, 2] / sum(confusion_1[2, ])
f1_score_1 <- 2 * (precision_1 * recall_1) / (precision_1 + recall_1)
precision_2 <- confusion_2[2, 2] / sum(confusion_2[, 2])
recall_2 <- confusion_2[2, 2] / sum(confusion_2[2, ])
f1_score_2 <- 2 * (precision_2 * recall_2) / (precision_2 + recall_2)
print(f1_score_1) # f1 score of bank in the training
print(f1_score_2) # f1 score of bank_grouped in the training
# Compute confusion matrix for rf_model_1
cm_1 <- confusionMatrix(predictions_1, testDataB$RESPONSE)
print(cm_1)
# Compute F1 score for rf_model_1
f1_1 <- cm_1$byClass["F1"]
# Compute confusion matrix for rf_model_2
cm_2 <- confusionMatrix(predictions_2, testDataBG$RESPONSE)
print(cm_2)
# Compute F1 score for rf_model_2
f1_2 <- cm_2$byClass["F1"]
# Print the F1 scores
print(f1_1) # f1 score of bank in the testing
print(f1_2) # f1 score of bank_grouped in the testing
# Apply the randomForest with Cross-Validation and Down-sampling
model_rf1 <- caret::train(RESPONSE ~ .,
data = trainDataB,
method = "rf",
preProcess = NULL,
trControl = trainControl(method = "repeatedcv", number = 10,
repeats = 10, verboseIter = FALSE, sampling = "down")
)
model_rf2 <- caret::train(RESPONSE ~ .,
data = trainDataBG,
method = "rf",
preProcess = NULL,
trControl = trainControl(method = "repeatedcv", number = 10,
repeats = 10, verboseIter = FALSE, sampling = "down")
)
model_test <-
# Make predictions on the testing data
predictions1 <- predict(model_rf1, newdata = testDataB)
predictions2 <- predict(model_rf2, newdata = testDataBG)
# Generate the confusion matrix on the testing
confusion_matrixRFB <- caret::confusionMatrix(predictions1, testDataB$RESPONSE)
confusion_matrixRFBG <- caret::confusionMatrix(predictions2, testDataBG$RESPONSE)
# Print the confusion matrix
print(confusion_matrixRFB) # 52.8%
print(confusion_matrixRFBG) # 48.5%
### Logistic Regression, Balanced
library(caret)
# Split the dataset into training and testing sets
set.seed(123)
trainIndex3 <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
trainData3 <- bank[trainIndex3, ]
testData3 <- bank[-trainIndex3, ]
trainIndex4 <- createDataPartition(bank_grouped$RESPONSE, p = 0.7, list = FALSE)
trainData4 <- bank_grouped[trainIndex4, ]
testData4 <- bank_grouped[-trainIndex4, ]
# Perform logistic regression with balanced data using caret
log_reg_model3 <- train(
RESPONSE ~ .,
data = trainData3,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10, sampling = "down")
)
log_reg_model4 <- train(
RESPONSE ~ .,
data = trainData4,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10, sampling = "down")
)
# Print the model details
summary(log_reg_model3)
summary(log_reg_model4)
# Predict using the trained logistic regression model
predictions3 <- predict(log_reg_model3, newdata = testData1)
predictions4 <- predict(log_reg_model4, newdata = testData2)
# Print the confusion matrix on testing
confusionMatrix(predictions3, testData3$RESPONSE)
confusionMatrix(predictions4, testData4$RESPONSE)
### Logistic Regression, Balanced
library(caret)
# Split the dataset into training and testing sets
set.seed(123)
trainIndex3 <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
trainData3 <- bank[trainIndex3, ]
testData3 <- bank[-trainIndex3, ]
trainIndex4 <- createDataPartition(bank_grouped$RESPONSE, p = 0.7, list = FALSE)
trainData4 <- bank_grouped[trainIndex4, ]
testData4 <- bank_grouped[-trainIndex4, ]
# Perform logistic regression with balanced data using caret
log_reg_model3 <- train(
RESPONSE ~ .,
data = trainData3,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10, sampling = "down")
)
log_reg_model4 <- train(
RESPONSE ~ .,
data = trainData4,
method = "glm",
family = "binomial",
trControl = trainControl(method = "cv", number = 10, sampling = "down")
)
# Print the model details
summary(log_reg_model3)
summary(log_reg_model4)
# Predict using the trained logistic regression model
predictions3 <- predict(log_reg_model3, newdata = testData1)
predictions4 <- predict(log_reg_model4, newdata = testData2)
# Print the confusion matrix on testing
confusionMatrix(predictions3, testData3$RESPONSE)
confusionMatrix(predictions4, testData4$RESPONSE)
library(rpart)
library(rpart.plot)
# Split the dataset into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(bank_grouped$RESPONSE, p = 0.7, list = FALSE)
trainData <- bank_grouped[trainIndex, ]
testData <- bank_grouped[-trainIndex, ]
# Train the classification tree model
classification_tree1 <- rpart(
RESPONSE ~ .,
data = trainData,
method = "class",
control = rpart.control(cp = 0.001)
)
# Prune the classification tree
pruned_tree <- prune(classification_tree1, cp = 0.01)  # Replace 0.01 with an appropriate value
summary(pruned_tree)
par(cex = 2)  # Adjust the text size
# Plot the pruned tree
rpart.plot(pruned_tree)
# Make predictions on the test dataset using the pruned tree
predictions <- predict(pruned_tree, newdata = testData, type = "class")
# Print the confusion matrix
CM2 <- confusionMatrix(predictions, testData$RESPONSE)
library(rpart)
library(rpart.plot)
# Split the dataset into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
trainData <- bank[trainIndex, ]
testData <- bank[-trainIndex, ]
# Train the classification tree model
classification_tree2 <- rpart(
RESPONSE ~ .,
data = trainData,
method = "class",
control = rpart.control(cp = 0.001)
)
# Prune the classification tree
prunedtree <- prune(classification_tree2, cp = 0.01)  # Replace 0.01 with an appropriate value
summary(prunedtree)
par(cex = 2)  # Adjust the text size
# Plot the pruned tree
rpart.plot(prunedtree)
# Make predictions on the test dataset using the pruned tree
predictions <- predict(prunedtree, newdata = testData, type = "class")
# Print the confusion matrix
CM1 <- confusionMatrix(predictions, testData$RESPONSE)
# Apply the randomForest with Cross-Validation and Down-sampling
model_rf1 <- caret::train(RESPONSE ~ .,
data = trainDataB,
method = "rf",
preProcess = NULL,
trControl = trainControl(method = "repeatedcv", number = 10,
repeats = 10, verboseIter = FALSE, sampling = "down")
)
model_rf2 <- caret::train(RESPONSE ~ .,
data = trainDataBG,
method = "rf",
preProcess = NULL,
trControl = trainControl(method = "repeatedcv", number = 10,
repeats = 10, verboseIter = FALSE, sampling = "down")
)
source(here::here("scripts/setup.R"))
source(here::here("scripts/setup.R"))
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
source(here::here("scripts/setup.R"))
source(here::here("scripts/setup.R"))
source(here::here("scripts/setup.R"))
source(here::here("scripts/setup.R"))
source(here::here("scripts/setup.R"))
source(here::here("scripts/setup.R"))
source(here::here("scripts/setup.R"))
library(pROC)
library(caret)
# Split the data into training and testing sets
set.seed(123)
train_idx <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
bank <- read.csv2(here("Data/GermanCredit.csv"))  # We need to use "csv2" because the variables are separated by semicolons
# load the required packages
packages <- c(
"tidyverse","knitr", "DataExplorer", "here", "DT", "corrplot", "dplyr",
"ggplot2", "Hmisc", "tidyquant", "lubridate",
"RColorBrewer", "psych", "GGally", "corrr", "ggcorrplot",
"kableExtra", "inspectdf", "skimr", "plotly", "heatmaply",
"ggcorrplot","mltools", "data.table", "randomForest", "caret", "rpart.plot",
"knitr", "kableExtra", "rmarkdown"
)
bank <- read.csv2(here("Data/GermanCredit.csv"))  # We need to use "csv2" because the variables are separated by semicolons
bank <- read.csv2(here("Data/GermanCredit.csv"))  # We need to use "csv2" because the variables are separated by semicolons
bank <- read.csv2("Data/GermanCredit.csv")  # We need to use "csv2" because the variables are separated by semicolons
bank <- read.csv2("GermanCredit.csv")  # We need to use "csv2" because the variables are separated by semicolons
knitr::opts_chunk$set(echo = TRUE)
library(pROC)
library(caret)
# Split the data into training and testing sets
set.seed(123)
train_idx <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
library(rpart)
# Split the dataset into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(bank$RESPONSE, p = 0.7, list = FALSE)
